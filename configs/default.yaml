# Default Configuration for Fred-LLM
# This is the standard configuration for development and testing

# Dataset settings
dataset:
  # Pre-split dataset paths (generated by scripts/prepare_dataset.py)
  train_path: data/processed/training_data/train_infix.csv
  val_path: data/processed/training_data/val_infix.csv
  test_path: data/processed/training_data/test_infix.csv
  format: null  # Auto-detected from filename or file content. Can specify: infix, latex, rpn
  max_samples: null  # Use all samples

# Model settings
model:
  provider: openai
  name: gpt-4o-mini
  temperature: 0.1
  max_tokens: 2048
  timeout: 60

# Prompting settings
prompting:
  # Option 1: Use pre-generated prompts (faster)
  prompts_dir: data/prompts/chain-of-thought  # Set to null to generate on-the-fly
  
  # Option 2: Generate on-the-fly from dataset (if prompts_dir is null)
  style: chain-of-thought  # basic, chain-of-thought, few-shot, tool-assisted
  edge_case_mode: none  # none, guardrails, hints
  num_examples: 2  # For few-shot style

# Evaluation settings
evaluation:
  mode: both  # symbolic, numeric, both
  symbolic_tolerance: 1e-10
  numeric_tolerance: 1e-6
  num_test_points: 100
  integration_domain:
    a: 0
    b: 1

# Output settings
output:
  dir: outputs
  save_predictions: true
  save_metrics: true
  log_level: INFO
