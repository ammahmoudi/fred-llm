# Local Model Configuration
# For running with local/self-hosted models (vLLM, Ollama, HuggingFace)

dataset:
  # Pre-split dataset paths
  train_path: data/processed/training_data/train_infix.csv
  val_path: data/processed/training_data/val_infix.csv
  test_path: data/processed/training_data/test_infix.csv
  format: infix  # infix, latex, or rpn
  max_samples: 100  # Moderate size for local testing

model:
  provider: local
  name: microsoft/Phi-3-mini-4k-instruct  # Or any local model
  endpoint: http://localhost:8000/v1  # vLLM/Ollama endpoint
  temperature: 0.1
  max_tokens: 2048
  timeout: 180  # Longer timeout for local models

prompting:
  # Pre-generated prompts directory
  prompts_dir: data/prompts/basic
  style: basic  # Simpler prompts work better with smaller models

evaluation:
  mode: both
  symbolic_tolerance: 1e-8
  numeric_tolerance: 1e-4
  num_test_points: 50
  integration_domain:
    a: 0
    b: 1

output:
  dir: outputs/local
  save_predictions: true
  save_metrics: true
  log_level: DEBUG
