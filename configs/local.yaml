# Local Model Configuration
# For running with local/self-hosted models (vLLM, Ollama, HuggingFace)

dataset:
  # Pre-split dataset paths
  prepared:
    train_path: data/processed/training_data/train_infix.csv
    val_path: data/processed/training_data/val_infix.csv
    test_path: data/processed/training_data/test_infix.csv
    format: infix  # infix, latex, or rpn
    max_samples: 100  # Moderate size for local testing
  
  # Generate prompts on-the-fly
  prompting:
    style: basic  # Simpler prompts work better with smaller models
    edge_case_mode: none
    num_examples: 2
    include_ground_truth: true

model:
  provider: local
  name: microsoft/Phi-3-mini-4k-instruct  # Or any local model
  base_url: http://localhost:8000/v1  # vLLM/Ollama endpoint (replaces 'endpoint' param)
  temperature: 0.1
  max_tokens: 2048
  timeout: 180  # Longer timeout for local models

evaluation:
  mode: both
  symbolic_tolerance: 1e-8
  numeric_tolerance: 1e-4
  num_test_points: 50
  type_tolerances:
    series: 1e-2
    approx_coef: 1e-3
    regularized: 1e-3

output:
  dir: outputs/local
  save_predictions: true
  save_metrics: true
  log_level: DEBUG
