# Fine-tuning Configuration
# Settings for fine-tuning experiments

dataset:
  # Pre-split dataset paths for fine-tuning
  prepared:
    train_path: data/processed/training_data/train_infix.csv
    val_path: data/processed/training_data/val_infix.csv
    test_path: data/processed/training_data/test_infix.csv
    format: infix  # infix, latex, or rpn
    max_samples: null
  
  # Use pre-generated basic prompts
  prompts:
    prompts_dir: data/prompts/basic
    style: basic

model:
  provider: huggingface
  name: google/flan-t5-base  # Or microsoft/phi-2
  temperature: 0.1
  max_tokens: 512
  timeout: 300

# Fine-tuning specific settings (for future implementation)
fine_tuning:
  enabled: true
  epochs: 3
  batch_size: 8
  learning_rate: 2e-5
  warmup_steps: 100
  save_steps: 500
  eval_steps: 100
  output_dir: outputs/fine_tuned
  fp16: true
  gradient_accumulation_steps: 4

evaluation:
  mode: both
  symbolic_tolerance: 1e-8
  numeric_tolerance: 1e-5
  num_test_points: 100
  type_tolerances:
    series: 1e-2
    approx_coef: 1e-3
    regularized: 1e-3

output:
  dir: outputs/fine_tuning
  save_predictions: true
  save_metrics: true
  log_level: INFO
