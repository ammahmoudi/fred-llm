# Fred-LLM Configuration
# Runtime settings for Fredholm integral equation solver

# Dataset settings
dataset:
  # Pre-split dataset paths (generated by scripts/prepare_dataset.py)
  train_path: "data/processed/training_data/train_infix.csv"
  val_path: "data/processed/training_data/val_infix.csv"
  test_path: "data/processed/training_data/test_infix.csv"
  format: null  # Auto-detected from filename or file content. Can specify: infix, latex, rpn
  max_samples: null  # null for all samples, or limit for testing

# Model settings
model:
  provider: "openai"  # openai, local, huggingface
  name: "gpt-4"
  api_key_env: "OPENAI_API_KEY"
  base_url: null  # for local models or custom endpoints
  temperature: 0.1
  max_tokens: 2048
  timeout: 60

# Prompt settings
prompting:
  # Option 1: Use pre-generated prompts (faster, recommended for production)
  prompts_dir: "data/prompts/chain-of-thought"  # Set to null to generate on-the-fly
  
  # Option 2: Generate on-the-fly from dataset (if prompts_dir is null)
  style: "chain-of-thought"  # basic, chain-of-thought, few-shot, tool-assisted
  edge_case_mode: "none"  # none, guardrails, hints
  num_examples: 2  # For few-shot style

# Evaluation settings
evaluation:
  mode: "both"  # symbolic, numeric, both
  symbolic_tolerance: 1e-10
  numeric_tolerance: 1e-6
  test_points: 100
  integration_method: "quad"  # quad, trapz, simpson

# Output settings
output:
  format: "latex"  # latex, rpn, python, sympy
  save_intermediate: true
  results_dir: "data/processed/results/"

# Logging settings
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_to_file: true
  log_dir: "logs/"

# Integral equation parameters
equation:
  kernel_types:
    - "polynomial"
    - "exponential"
    - "trigonometric"
    - "separable"
  domain:
    a: 0.0
    b: 1.0
  lambda_range:
    min: 0.1
    max: 2.0
